{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# better_rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.np import *  # import numpy as np\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class BetterRnnlm(BaseModel):\n",
    "    '''\n",
    "     LSTM 계층을 2개 사용하고 각 층에 드롭아웃을 적용한 모델이다.\n",
    "     아래 [1]에서 제안한 모델을 기초로 하였고, [2]와 [3]의 가중치 공유(weight tying)를 적용했다.\n",
    "     [1] Recurrent Neural Network Regularization (https://arxiv.org/abs/1409.2329)\n",
    "     [2] Using the Output Embedding to Improve Language Models (https://arxiv.org/abs/1608.05859)\n",
    "     [3] Tying Word Vectors and Word Classifiers (https://arxiv.org/pdf/1611.01462.pdf)\n",
    "    '''\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=650,\n",
    "                 hidden_size=650, dropout_ratio=0.5):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
    "        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeAffine(embed_W.T, affine_b)  # weight tying!!\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
    "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs, train_flg=False):\n",
    "        for layer in self.drop_layers:\n",
    "            layer.train_flg = train_flg\n",
    "\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts, train_flg=True):\n",
    "        score = self.predict(xs, train_flg)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        for layer in self.lstm_layers:\n",
    "            layer.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clip_grads.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [0.58817817 8.5091589  9.53590768 6.17992323 6.67375126 8.76755935\n",
      " 2.10070354 7.22327021 4.03093717]\n",
      "after: [0.10543427 1.52531491 1.70936544 1.10778623 1.1963077  1.57163465\n",
      " 0.376563   1.29481208 0.72256831]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dW1 = np.random.rand(3, 3) * 10\n",
    "dW2 = np.random.rand(3, 3) * 10\n",
    "grads = [dW1, dW2]\n",
    "max_norm = 5.0\n",
    "\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "\n",
    "print('before:', dW1.flatten())\n",
    "clip_grads(grads, max_norm)\n",
    "print('after:', dW1.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Rnnlm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d075374862f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRnnlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#model = BetterRnnlm()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Rnnlm' is not defined"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from rnnlm import Rnnlm\n",
    "# from better_rnnlm import BetterRnnlm\n",
    "from dataset import ptb\n",
    "from common.util import eval_perplexity\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = Rnnlm()\n",
    "    #model = BetterRnnlm()\n",
    "\n",
    "    # 학습된 매개변수 읽기\n",
    "    model.load_params()\n",
    "\n",
    "    corpus, _, _ = ptb.load_data('test')\n",
    "\n",
    "    model.reset_state()\n",
    "    ppl_test = eval_perplexity(model, corpus)\n",
    "    print('test perplexity: ', ppl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn_gradient_graph.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4684068094579303, 3.3357049741610365, 4.783279375373182, 6.279587332087612, 8.080776465019053, 10.251163032292936, 12.936063506609896, 16.276861327786712, 20.45482961834598, 25.688972842084684, 32.25315718048336, 40.48895641683869, 50.8244073070191, 63.79612654485427, 80.07737014308985, 100.5129892205125, 126.16331847536823, 158.35920648258823, 198.7710796761195, 249.495615421267]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53356 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45432 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47492 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53356 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45432 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47492 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 2   # 미니배치 크기\n",
    "H = 3   # 은닉 상태 벡터의 차원 수\n",
    "T = 20  # 시계열 데이터의 길이\n",
    "\n",
    "dh = np.ones((N, H))\n",
    "\n",
    "np.random.seed(3) # 재현할 수 있도록 난수의 시드 고정\n",
    "\n",
    "Wh = np.random.randn(H, H)\n",
    "#Wh = np.random.randn(H, H) * 0.5\n",
    "\n",
    "norm_list = []\n",
    "for t in range(T):\n",
    "    dh = np.dot(dh, Wh.T)\n",
    "    norm = np.sqrt(np.sum(dh**2)) / N\n",
    "    norm_list.append(norm)\n",
    "\n",
    "print(norm_list)\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(np.arange(len(norm_list)), norm_list)\n",
    "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
    "plt.xlabel('시간 크기(time step)')\n",
    "plt.ylabel('노름(norm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class Rnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 10873141858082210777,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 13221628684810083857\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 2932825752538851779\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 7167360384\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 5644615029899939047\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_better_rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 1327 | 시간 0[s] | 퍼플렉서티 10000.17\n",
      "| 에폭 1 |  반복 21 / 1327 | 시간 16[s] | 퍼플렉서티 4646.49\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common import config\n",
    "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
    "# ==============================================\n",
    "# config.GPU = True\n",
    "# ==============================================\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity, to_gpu\n",
    "from dataset import ptb\n",
    "# from better_rnnlm import BetterRnnlm\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 650\n",
    "hidden_size = 650\n",
    "time_size = 35\n",
    "lr = 20.0\n",
    "max_epoch = 40\n",
    "max_grad = 0.25\n",
    "dropout = 0.5\n",
    "\n",
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_val, _, _ = ptb.load_data('val')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "\n",
    "if config.GPU:\n",
    "    corpus = to_gpu(corpus)\n",
    "    corpus_val = to_gpu(corpus_val)\n",
    "    corpus_test = to_gpu(corpus_test)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "best_ppl = float('inf')\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n",
    "                time_size=time_size, max_grad=max_grad)\n",
    "\n",
    "    model.reset_state()\n",
    "    ppl = eval_perplexity(model, corpus_val)\n",
    "    print('검증 퍼플렉서티: ', ppl)\n",
    "\n",
    "    if best_ppl > ppl:\n",
    "        best_ppl = ppl\n",
    "        model.save_params()\n",
    "    else:\n",
    "        lr /= 4.0\n",
    "        optimizer.lr = lr\n",
    "\n",
    "    model.reset_state()\n",
    "    print('-' * 50)\n",
    "\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('테스트 퍼플렉서티: ', ppl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity\n",
    "from dataset import ptb\n",
    "# from rnnlm import Rnnlm\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNN의 은닉 상태 벡터의 원소 수\n",
    "time_size = 35     # RNN을 펼치는 크기\n",
    "lr = 20.0\n",
    "max_epoch = 4\n",
    "max_grad = 0.25\n",
    "\n",
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "# 모델 생성\n",
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "# 기울기 클리핑을 적용하여 학습\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n",
    "            eval_interval=20)\n",
    "trainer.plot(ylim=(0, 500))\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('테스트 퍼플렉서티: ', ppl_test)\n",
    "\n",
    "# 매개변수 저장\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
